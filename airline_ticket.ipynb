{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPM5rpELnBD2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qQRxv6fSC1m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/project machine learning/Indian Airlines.csv')"
      ],
      "metadata": {
        "id": "0SAsbzXslGHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "f49yfSdBlpt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "2UVqm-ovlstW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "zPazaeLgl0eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "T9zy8eUCl4cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Ys1ou9Mjl6pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "tnH8YfPQl-Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "bzKx-3ChmAdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['price']=np.log1p(df['price'])"
      ],
      "metadata": {
        "id": "eR-1Is4t0QoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.columns:\n",
        "  if df[i].dtype == 'object':\n",
        "    print(i)\n",
        "    print(len(df[i].unique()))"
      ],
      "metadata": {
        "id": "EXh079qmF4B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['flight'],inplace=True)\n"
      ],
      "metadata": {
        "id": "GBzGtVgiG9o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "\n",
        "le.fit(df['class'])\n",
        "df['class'] = le.transform(df['class'])"
      ],
      "metadata": {
        "id": "2Yx3hoQmmXMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe1 = OneHotEncoder(sparse_output=False)  # drop=\"first\" prevents dummy variable trap\n",
        "airline_encoded = ohe1.fit_transform(df[[\"airline\"]])\n",
        "# Convert to DataFrame with column names\n",
        "airline_df = pd.DataFrame(airline_encoded, columns=ohe1.get_feature_names_out([\"airline\"]))\n",
        "# Drop original \"Airline\" column and merge encoded data\n",
        "df = df.drop(columns=[\"airline\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, airline_df], axis=1)\n"
      ],
      "metadata": {
        "id": "gLqhnBgjnOsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe3 = OneHotEncoder(sparse_output=False)\n",
        "source = ohe3.fit_transform(df[[\"source_city\"]])\n",
        "source_df = pd.DataFrame(source, columns=ohe3.get_feature_names_out([\"source_city\"]))\n",
        "df = df.drop(columns=[\"source_city\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, source_df], axis=1)"
      ],
      "metadata": {
        "id": "Ev2kB55xrZPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe3"
      ],
      "metadata": {
        "id": "14DY_-SE46bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe4 = OneHotEncoder(sparse_output=False)\n",
        "dep = ohe4.fit_transform(df[[\"departure_time\"]])\n",
        "dep_df = pd.DataFrame(dep, columns=ohe4.get_feature_names_out([\"departure_time\"]))\n",
        "df = df.drop(columns=[\"departure_time\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, dep_df], axis=1)"
      ],
      "metadata": {
        "id": "WvUOx9SDtM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe5 = OneHotEncoder(sparse_output=False)\n",
        "sto = ohe5.fit_transform(df[[\"stops\"]])\n",
        "sto_df = pd.DataFrame(sto, columns=ohe5.get_feature_names_out([\"stops\"]))\n",
        "df = df.drop(columns=[\"stops\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, sto_df], axis=1)"
      ],
      "metadata": {
        "id": "iPp05Z8Zuq6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe6 = OneHotEncoder(sparse_output=False)\n",
        "at = ohe6.fit_transform(df[[\"arrival_time\"]])\n",
        "at_df = pd.DataFrame(at, columns=ohe6.get_feature_names_out([\"arrival_time\"]))\n",
        "df = df.drop(columns=[\"arrival_time\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, at_df], axis=1)"
      ],
      "metadata": {
        "id": "Vz-D7qBGMdgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe7 = OneHotEncoder(sparse_output=False)\n",
        "destination = ohe7.fit_transform(df[[\"destination_city\"]])\n",
        "destination_df = pd.DataFrame(destination, columns=ohe7.get_feature_names_out([\"destination_city\"]))\n",
        "df = df.drop(columns=[\"destination_city\"]).reset_index(drop=True)\n",
        "df = pd.concat([df, destination_df], axis=1)"
      ],
      "metadata": {
        "id": "Nm8opyhkM7Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to create box plots for\n",
        "columns = ['duration', 'days_left', 'price']\n",
        "\n",
        "# Iterate through the columns and create box plots\n",
        "for column in columns:\n",
        "    plt.figure()  # Create a new figure for each plot\n",
        "    sns.boxplot(x=df[column])\n",
        "    plt.title(f'Box Plot of {column}')  # Set title using f-string formatting\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dmw-df2pNhVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['duration', 'days_left', 'price']\n",
        "\n",
        "for column in columns:\n",
        "  Q1 = df[column].quantile(0.25)\n",
        "  Q3 = df[column].quantile(0.75)\n",
        "  print(f\"Column: {column}\")\n",
        "  print(f\"Lower Quartile (Q1): {Q1}\")\n",
        "  print(f\"Upper Quartile (Q3): {Q3}\")\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "m_Ke62CY7yhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['duration', 'days_left', 'price']\n",
        "\n",
        "for column in columns:\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
        "\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Number of outliers: {len(outliers)}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "kJmRbsgG814w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df.sort_values(by=['price'], ascending=False)\n",
        "df_sorted.head() # To view the first few rows of the sorted DataFrame\n"
      ],
      "metadata": {
        "id": "JEtvqSQM9MfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy() # Call the copy method using parentheses ()\n",
        "df1[df['price']>100000]"
      ],
      "metadata": {
        "id": "iPge4z-7BdDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1[df['price'] <= 100000]\n"
      ],
      "metadata": {
        "id": "d6jgRjVt-CX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[df['price']>100000]"
      ],
      "metadata": {
        "id": "y7cVe3Ji-xAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1[df['duration'] > 44]\n"
      ],
      "metadata": {
        "id": "HPRtws5t_NJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "9HWbe5ou_AA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "N7g61xwA_mrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1[df['duration'] < 44]\n"
      ],
      "metadata": {
        "id": "xlcrv1kE_0Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "UwhUAfVvAj4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape\n"
      ],
      "metadata": {
        "id": "yn-mouSgAltN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn #Install imbalanced-learn, which contains imblearn"
      ],
      "metadata": {
        "id": "ut4PazRoD8jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "gDCdaAwAAvpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y)\n",
        "X = df.drop(columns=['price'])\n",
        "y = df['price']"
      ],
      "metadata": {
        "id": "UeldZndqCzfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "zXGH6IrNGjlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=10000,replace=False,random_state=42)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "Ym3GzusSTW8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "uG5U59YPE7J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation = df.corr()\n",
        "plt.figure(figsize=(25,25))\n",
        "sns.heatmap(correlation, annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9c9uGAuQDrw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "46EkG20LEzyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "id": "e83kBl5CFRO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "ctz1zvi1FsLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both train and test data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Mrpwq3YdFpxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "EJ7fG909WHHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ... (your code for training and prediction) ...\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "fU8uPrcxWQIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".)decision tree"
      ],
      "metadata": {
        "id": "eSnM_p8_BsvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Assuming x_train_scaled, y_train are already defined\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution\n",
        "param_dist_dt = {\n",
        "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 6],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search\n",
        "random_search_dt = RandomizedSearchCV(\n",
        "    estimator=dt,\n",
        "    param_distributions=param_dist_dt,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,  # Use all available cores for parallel processing\n",
        "    random_state=42  #Adding this for reproducibility to give the same result as the example you provided\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_dt = random_search_dt.best_estimator_\n",
        "best_params_dt = random_search_dt.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params_dt}\")\n",
        "\n",
        "# Now you have the same model as before, stored in 'best_model_dt'"
      ],
      "metadata": {
        "id": "CduJBQWshyuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Assuming best_model_dt, X_test_scaled, and y_test are already defined\n",
        "y_pred_dt = best_model_dt.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_dt = np.mean(np.abs((y_test - y_pred_dt) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Decision Tree - Mean Squared Error (MSE): {mse_dt}\")\n",
        "print(f\"Decision Tree - Mean Absolute Error (MAE): {mae_dt}\")\n",
        "print(f\"Decision Tree - Mean Absolute Percentage Error (MAPE): {mape_dt}\")\n",
        "print(f\"Decision Tree - R-squared (R2): {r2_dt}\")"
      ],
      "metadata": {
        "id": "CSMvJPyjdL3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " .)Random Forest"
      ],
      "metadata": {
        "id": "-ZkfQHnBBMXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Assuming X_train_scaled, y_train are already defined\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution for Random Forest\n",
        "param_dist_rf = {\n",
        "    'n_estimators': randint(50, 200),  # Number of trees in the forest\n",
        "    'max_depth': [10, 20, 30, 40, 50, None],\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]  # Whether bootstrap samples are used\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for Random Forest\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist_rf,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,  # Use all available cores for parallel processing\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_rf = random_search_rf.best_estimator_\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for Random Forest: {best_params_rf}\")"
      ],
      "metadata": {
        "id": "68sFlkSaqmf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Assuming best_model_rf, X_test_scaled, and y_test are already defined\n",
        "y_pred_rf = best_model_rf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_rf = np.mean(np.abs((y_test - y_pred_rf) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Random Forest - Mean Squared Error (MSE): {mse_rf}\")\n",
        "print(f\"Random Forest - Mean Absolute Error (MAE): {mae_rf}\")\n",
        "print(f\"Random Forest - Mean Absolute Percentage Error (MAPE): {mape_rf}\")\n",
        "print(f\"Random Forest - R-squared (R2): {r2_rf}\")"
      ],
      "metadata": {
        "id": "PGlLUiVSD8Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Linear Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "GQHEFXfZBPaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, and y_test are already defined\n",
        "\n",
        "# Initialize Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Define the hyperparameter distribution (Limited options for Linear Regression)\n",
        "param_dist_linear = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'positive': [True, False]  # Constraint coefficients to be positive\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for Linear Regression\n",
        "random_search_linear = RandomizedSearchCV(\n",
        "    estimator=linear_model,\n",
        "    param_distributions=param_dist_linear,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_linear.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_linear = random_search_linear.best_estimator_\n",
        "best_params_linear = random_search_linear.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for Linear Regression: {best_params_linear}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_linear = best_model_linear.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_linear = mean_absolute_error(y_test, y_pred_linear)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_linear = np.mean(np.abs((y_test - y_pred_linear) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_linear = r2_score(y_test, y_pred_linear)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Linear Regression - Mean Squared Error (MSE): {mse_linear}\")\n",
        "print(f\"Linear Regression - Mean Absolute Error (MAE): {mae_linear}\")\n",
        "print(f\"Linear Regression - Mean Absolute Percentage Error (MAPE): {mape_linear}\")\n",
        "print(f\"Linear Regression - R-squared (R2): {r2_linear}\")"
      ],
      "metadata": {
        "id": "9Njxa9g8cZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Lasso Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "GNdBSlWJBTmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import expon\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, and y_test are already defined\n",
        "\n",
        "# Initialize Lasso Regression\n",
        "lasso_model = Lasso(random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution for Lasso\n",
        "param_dist_lasso = {\n",
        "    'alpha': expon(scale=1.0),  # Regularization strength (alpha)\n",
        "    'selection': ['cyclic', 'random']  # Feature selection method\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for Lasso\n",
        "random_search_lasso = RandomizedSearchCV(\n",
        "    estimator=lasso_model,\n",
        "    param_distributions=param_dist_lasso,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_lasso = random_search_lasso.best_estimator_\n",
        "best_params_lasso = random_search_lasso.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for Lasso Regression: {best_params_lasso}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_lasso = best_model_lasso.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_lasso = np.mean(np.abs((y_test - y_pred_lasso) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Lasso Regression - Mean Squared Error (MSE): {mse_lasso}\")\n",
        "print(f\"Lasso Regression - Mean Absolute Error (MAE): {mae_lasso}\")\n",
        "print(f\"Lasso Regression - Mean Absolute Percentage Error (MAPE): {mape_lasso}\")\n",
        "print(f\"Lasso Regression - R-squared (R2): {r2_lasso}\")"
      ],
      "metadata": {
        "id": "p2lCTzRGcvAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3c1biP7cg8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ridge Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "zga9Ga_aBbNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import expon\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, and y_test are already defined\n",
        "\n",
        "# Initialize Ridge Regression\n",
        "ridge_model = Ridge(random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution for Ridge\n",
        "param_dist_ridge = {\n",
        "    'alpha': expon(scale=1.0),  # Regularization strength (alpha)\n",
        "    'solver': ['svd', 'cholesky', 'lsqr', 'sag', 'saga']  # Solver to use\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for Ridge\n",
        "random_search_ridge = RandomizedSearchCV(\n",
        "    estimator=ridge_model,\n",
        "    param_distributions=param_dist_ridge,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_ridge = random_search_ridge.best_estimator_\n",
        "best_params_ridge = random_search_ridge.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for Ridge Regression: {best_params_ridge}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_ridge = best_model_ridge.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_ridge = np.mean(np.abs((y_test - y_pred_ridge) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Ridge Regression - Mean Squared Error (MSE): {mse_ridge}\")\n",
        "print(f\"Ridge Regression - Mean Absolute Error (MAE): {mae_ridge}\")\n",
        "print(f\"Ridge Regression - Mean Absolute Percentage Error (MAPE): {mape_ridge}\")\n",
        "print(f\"Ridge Regression - R-squared (R2): {r2_ridge}\")"
      ],
      "metadata": {
        "id": "3X4fQVemdQ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. SVM\n",
        "\n"
      ],
      "metadata": {
        "id": "7htSp11HBesb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import expon, uniform\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, and y_test are already defined\n",
        "\n",
        "# Initialize SVM Regressor\n",
        "svm_model = SVR()\n",
        "\n",
        "# Define the hyperparameter distribution for SVM\n",
        "param_dist_svm = {\n",
        "    'C': expon(scale=1.0),  # Regularization parameter\n",
        "    'epsilon': expon(scale=0.1),  # Width of the epsilon-tube\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel type\n",
        "    'gamma': expon(scale=0.1),  # Kernel coefficient\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for SVM\n",
        "random_search_svm = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=param_dist_svm,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_svm = random_search_svm.best_estimator_\n",
        "best_params_svm = random_search_svm.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for SVM: {best_params_svm}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_svm = best_model_svm.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_svm = np.mean(np.abs((y_test - y_pred_svm) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_svm = r2_score(y_test, y_pred_svm)\n",
        "\n",
        "# Print the results\n",
        "print(f\"SVM - Mean Squared Error (MSE): {mse_svm}\")\n",
        "print(f\"SVM - Mean Absolute Error (MAE): {mae_svm}\")\n",
        "print(f\"SVM - Mean Absolute Percentage Error (MAPE): {mape_svm}\")\n",
        "print(f\"SVM - R-squared (R2): {r2_svm}\")"
      ],
      "metadata": {
        "id": "h4XKNFhcdusQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Gradient Boosting\n",
        "\n"
      ],
      "metadata": {
        "id": "0NaCJe_tBgEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train_scaled, y_train, X_test_scaled, and y_test are already defined\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter distribution for Gradient Boosting\n",
        "param_dist_gb = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "}\n",
        "\n",
        "# Initialize Randomized Search for Gradient Boosting\n",
        "random_search_gb = RandomizedSearchCV(\n",
        "    estimator=gb_model,\n",
        "    param_distributions=param_dist_gb,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit Randomized Search to the data\n",
        "random_search_gb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_gb = random_search_gb.best_estimator_\n",
        "best_params_gb = random_search_gb.best_params_\n",
        "\n",
        "print(f\"Best Hyperparameters for Gradient Boosting: {best_params_gb}\")\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_gb = best_model_gb.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape_gb = np.mean(np.abs((y_test - y_pred_gb) / y_test)) * 100\n",
        "\n",
        "# Calculate R2\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Gradient Boosting - Mean Squared Error (MSE): {mse_gb}\")\n",
        "print(f\"Gradient Boosting - Mean Absolute Error (MAE): {mae_gb}\")\n",
        "print(f\"Gradient Boosting - Mean Absolute Percentage Error (MAPE): {mape_gb}\")\n",
        "print(f\"Gradient Boosting - R-squared (R2): {r2_gb}\")"
      ],
      "metadata": {
        "id": "KZ7hejVCeDwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "GJKMhhIGu2rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(best_model_linear,open('best_model_linear.sav','wb'))"
      ],
      "metadata": {
        "id": "lgUnIym99COw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(le,open('le.sav','wb'))"
      ],
      "metadata": {
        "id": "gRU-kOWRAB2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe1,open('ohe1.sav','wb'))"
      ],
      "metadata": {
        "id": "V2mxy7TxFYrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe3,open('ohe3.sav','wb'))"
      ],
      "metadata": {
        "id": "yejcLnToFsD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe4,open('ohe4.sav','wb'))"
      ],
      "metadata": {
        "id": "KHUz14QTHltz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe5,open('ohe5.sav','wb'))"
      ],
      "metadata": {
        "id": "ufksmRO8H25x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe6,open('ohe6.sav','wb'))"
      ],
      "metadata": {
        "id": "sQxUI5sBH6XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(ohe7,open('ohe7.sav','wb'))"
      ],
      "metadata": {
        "id": "xLKuQ7GtH9WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(scaler,open('scaler.sav','wb'))"
      ],
      "metadata": {
        "id": "vYOv6NPtH_1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "s3Zi-QdvIMCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}